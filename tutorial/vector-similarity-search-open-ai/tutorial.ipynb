{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct vector similarity search on Azure OpenAI embeddings using Azure Managed Redis\n",
    "\n",
    "- Tutorial: https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-tutorial-vector-similarity\n",
    "- Code: https://github.com/Azure-Samples/azure-cache-redis-samples/tree/main/tutorial/vector-similarity-search-open-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "Install the python dependencies required for our application. Using a Python virtual environment is usually a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell 1\n",
    "\n",
    "! pip install openai num2words matplotlib plotly scipy scikit-learn pandas tiktoken redis langchain langchain_openai langchain_community langchain-redis\n",
    "! pip install langchain-huggingface sentence-transformers scikit-learn\n",
    "# ! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set up Azure OpenAI and Azure Managed Redis connection info\n",
    "Fill in your Azure OpenAI and Azure Managed Redis information below. This will be used later to establish the connection these services, generate the embeddings, and load them into Redis. This example stores these values in application variables for the sake of simplicity. Outside of tutorials, it's strongly recommended to store these in environment variables or using a secrets manager like Azure KeyVault. \n",
    "\n",
    "Note that there are differences  between the `OpenAI` and `Azure OpenAI` endpoints. This example uses the configuration for `Azure OpenAI`. See [How to switch between OpenAI and Azure OpenAI endpoints with Python](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell 2\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from num2words import num2words\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Redis as RedisVectorStore\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "RESOURCE_ENDPOINT = os.getenv('RESOURCE_ENDPOINT')\n",
    "DEPLOYMENT_NAME = os.getenv('DEPLOYMENT_NAME')\n",
    "MODEL_NAME = os.getenv('MODEL_NAME')\n",
    "REDIS_ENDPOINT = os.getenv('REDIS_ENDPOINT')\n",
    "REDIS_PASSWORD = os.getenv('REDIS_PASSWORD')\n",
    "\n",
    "print(f\"RESOURCE_ENDPOINT: {RESOURCE_ENDPOINT}\")\n",
    "print(f\"REDIS_ENDPOINT: {REDIS_ENDPOINT}\")\n",
    "print(f\"DEPLOYMENT_NAME: {DEPLOYMENT_NAME}\")\n",
    "print(f\"MODEL_NAME: {MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset\n",
    "\n",
    "This example uses the [Wikipedia Movie Plots](https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots) dataset from Kaggle. Download this file and place it in the same directory as this jupyter notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell 3\n",
    "df=pd.read_csv(os.path.join(os.getcwd(),'wiki_movie_plots_deduped.csv'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the dataset to remove spaces in the column titles and filter the dataset to lower the size. This isn't required, but is helpful in reducing the time it takes to generate embeddings and loading the index into Redis. Feel free to play around with the filters, or add your own! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell 4\n",
    "\n",
    "df.insert(0, 'id', range(0, len(df)))\n",
    "df['year'] = df['Release Year'].astype(int)\n",
    "df['origin'] = df['Origin/Ethnicity'].astype(str)\n",
    "del df['Release Year']\n",
    "del df['Origin/Ethnicity']\n",
    "df = df[df.year > 1970] # only movies made after 1970\n",
    "df = df[df.origin.isin(['American','British','Canadian'])] # only movies from English-speaking cinema\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove whitespace from the `Plot` column to make it easier to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell 5\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# s is input text\n",
    "def normalize_text(s, sep_token = \" \\n \"):\n",
    "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
    "    s = re.sub(r\". ,\",\"\",s)\n",
    "    # remove all instances of multiple spaces\n",
    "    s = s.replace(\"..\",\".\")\n",
    "    s = s.replace(\". .\",\".\")\n",
    "    s = s.replace(\"\\n\", \"\")\n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "df['Plot']= df['Plot'].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of tokens required to generate the embeddings for this dataset. You may want to filter the dataset more stringently in order to limit the tokens required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell 6\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "df['n_tokens'] = df[\"Plot\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "df = df[df.n_tokens<8192]\n",
    "print('Number of movies: ' + str(len(df))) # print number of movies remaining in dataset\n",
    "print('Number of tokens required:' + str(df['n_tokens'].sum())) # print number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataframe into LangChain\n",
    "Using the `DataFrameLoader` class allows you to load a pandas dataframe into LangChain. That makes it easy to load your data and use it to generate embeddings using LangChain's other integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell 7\n",
    "\n",
    "loader = DataFrameLoader(df, page_content_column=\"Plot\" )\n",
    "movie_list = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings and Load them into Azure Managed Redis\n",
    "Using LangChain, this example connects to Azure OpenAI Service to generate embeddings for the dataset. These embeddings are then loaded into [Azure Managed Redis](https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/managed-redis/managed-redis-overview), a fully managed Redis service on Azure, which features the [RediSearch](https://redis.io/docs/latest/develop/interact/search-and-query/) module that includes vector search capability. Finally, a copy of the index schema is saved. That is useful for loading the index into Redis later if you don't want to regenerate the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell 8\n",
    "\n",
    "# we will use Azure OpenAI as our embeddings provider\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=RESOURCE_ENDPOINT,\n",
    "    azure_deployment=DEPLOYMENT_NAME,\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_version='2024-03-01-preview',\n",
    "    show_progress_bar=True,\n",
    "    chunk_size=16)\n",
    "\n",
    "# name of the Redis search index to create\n",
    "index_name = \"movieindex\"\n",
    "\n",
    "# create a connection string for the Redis Vector Store. Uses Redis-py format: https://redis-py.readthedocs.io/en/stable/connections.html#redis.Redis.from_url\n",
    "# This example assumes TLS is enabled. If not, use \"redis://\" instead of \"rediss://\n",
    "redis_url = \"rediss://:\" + REDIS_PASSWORD + \"@\"+ REDIS_ENDPOINT\n",
    "\n",
    "# Take the first 100 movies\n",
    "# short_list = movie_list[:100]\n",
    "\n",
    "# create and load redis with documents\n",
    "vectorstore = RedisVectorStore.from_documents(\n",
    "    documents=movie_list,\n",
    "    embedding=embedding,\n",
    "    index_name=index_name,\n",
    "    redis_url=redis_url\n",
    ")\n",
    "\n",
    "# save index schema so you can reload in the future without re-generating embeddings\n",
    "vectorstore.write_schema(\"redis_schema.yaml\")\n",
    "\n",
    "# This may take up to 10 minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run search queries\n",
    "Using the vectorstore we just built in LangChain, we can conduct similarity searches using the `similarity_search_with_score` method. In this example, the top 10 results for a given query are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell 9\n",
    "\n",
    "results = vectorstore.similarity_search_with_score(query=\"Spaceships, aliens, and heroes saving America\", k=10)\n",
    "\n",
    "for doc, score  in enumerate(results):\n",
    "    movie_title = str(results[doc][0].metadata['Title'])\n",
    "    similarity_score = str(round((1 - results[doc][1]),4))\n",
    "    print(movie_title + ' (Score: ' + similarity_score + ')')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run hybrid queries\n",
    "\n",
    "You can also run hybrid queries. That is, queries that use both vector search and filters based on other parameters in the dataset. In this case, we filter our query results to only movies tagged with the `comedy` genre. One of the advantages of using LangChain with Redis is that metadata is preserved in the index, so you can use it to filter your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell 10\n",
    "\n",
    "from langchain.vectorstores.redis import RedisText\n",
    "\n",
    "query = \"Spaceships, aliens, and heroes saving America\"\n",
    "genre = \"comedy\"\n",
    "\n",
    "genre_filter = RedisText(\"Genre\") == genre\n",
    "\n",
    "results = vectorstore.similarity_search_with_score(query, filter=genre_filter, k=10)\n",
    "for i, j in enumerate(results):\n",
    "    movie_title = str(results[i][0].metadata['Title'])\n",
    "    similarity_score = str(round((1 - results[i][1]),4))\n",
    "    print(movie_title + ' (Score: ' + similarity_score + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A: Load index data already in Redis\n",
    "If you already have embeddings data in Redis, you can load it into your LangChain vectorstore oboject using the `from_existing_index` method. This is useful if you don't want to re-run your embeddings model. You'll need to provide the index schema that was saved when you generated the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell 11\n",
    "\n",
    "# we will use Azure OpenAI as our embeddings provider\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=RESOURCE_ENDPOINT,\n",
    "    azure_deployment=DEPLOYMENT_NAME,\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_version='2024-03-01-preview',\n",
    "    show_progress_bar=True,\n",
    "    chunk_size=16)\n",
    "\n",
    "# name of the Redis search index to create\n",
    "index_name = \"movieindex\"\n",
    "\n",
    "# create a connection string for the Redis Vector Store. Uses Redis-py format: https://redis-py.readthedocs.io/en/stable/connections.html#redis.Redis.from_url\n",
    "# This example assumes TLS is enabled. If not, use \"redis://\" instead of \"rediss://\n",
    "redis_url = \"rediss://:\" + REDIS_PASSWORD + \"@\"+ REDIS_ENDPOINT\n",
    "\n",
    "vectorstore = RedisVectorStore.from_existing_index(\n",
    "    embedding=embedding,\n",
    "    redis_url=redis_url,\n",
    "    index_name=index_name,\n",
    "    schema=\"redis_schema.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix B: Query Redis using the CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install RedisVL\n",
    "! pip install redisvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Azure Manager Redis\n",
    "! rvl index listall -u $redis_url\n",
    "! rvl index info -i movieindex -u $redis_url\n",
    "! rvl stats -i movieindex -u $redis_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destroy movie index\n",
    "! rvl index destroy -i movieindex -u $redis_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix C: Simple RAG Chain (no chat hostory or question rewriting)\n",
    "\n",
    "See `movie-chat.py` and `movie-chat-ui.py` for chat history and question rewriting (history aware retriever usage).\n",
    "\n",
    "```sh\n",
    "python movie-chat.py\n",
    "# or\n",
    "streamlit run movie-chat-ui.py\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"RESOURCE_ENDPOINT\"),\n",
    "    azure_deployment='gpt-4o-mini',\n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    "    openai_api_version=\"2024-09-01-preview\"\n",
    ")\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"You are a movie buff who can answer questions about movies, make suggestions, summarise key facts, and provide other useful movie information. Use the following information as context to build your answer. If you are unsure, just say 'I'm unsure\".  Only discuss movies from the context provided.  Don't discuss other topics not related to the movies.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "while True:\n",
    "    question = input(\"What's your question about movie(s)? \")\n",
    "    if question == 'q' or question == '':\n",
    "        print('Bye!')\n",
    "        break\n",
    "    else:\n",
    "        answer = rag_chain.invoke(question)\n",
    "\n",
    "        print(f'\\nAnswer:\\n{answer}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
